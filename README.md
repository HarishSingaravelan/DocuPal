# ğŸ§  DocuPal â€” Intelligent PDF Assistant using Ollama & Streamlit

![ChatDoc Demo](static/ui.gif)

> A private, intelligent PDF chatbot that allows users to upload any document and ask questions about its content in natural language. Powered by **Ollama**, **LangChain**, **ChromaDB**, and **Streamlit**.

---

## ğŸš€ Features

- ğŸ“ Upload any PDF
- ğŸ’¬ Ask questions and get answers from the file contents
- ğŸ§  LLM-powered via Ollama (local model)
- ğŸ” Contextual RAG using ChromaDB
- ğŸ’¡ Fast and lightweight UI with Streamlit

---

## âš™ï¸ Setup Instructions

### ğŸ“¦ Option 1: Using Docker (Recommended)

> Docker image includes everything: Ollama + Streamlit + dependencies.

1. **Install Docker**: [Get Docker](https://www.docker.com/products/docker-desktop/)

2. **Clone the Repo**:
```
git clone https://github.com/HarishSingaravelan/DocuPal.git
cd DocuPal
```

3. **Create the image from Dockerfile**
```
docker build -t ollama-chat .
```
4. **Create a container with mounted volumes** (to persist Ollama models across runs and avoid re-downloading them each time the container starts)

```
docker run -v ollama-data:/root/.ollama -p 8501:8501 ollama-chat
```
> You can access streamlit via [localhost:8501](http://localhost:8501/)

### âš’ï¸ Option 2: Manual Setup (No Docker)
Make sure you have Python 3.9+, Ollama installed locally, and dependencies ready.

1. Install Ollama locally: https://ollama.com

2. Pull a supported model:
```
ollama pull llama3.2 mxbai-embed-large
```
3. Clone the repo:
```
git clone https://github.com/HarishSingaravelan/DocuPal.git
cd DocuPal
```
4. Create and activate a virtual environment:
```
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```
5. Install dependencies:
```
pip install -r requirements.txt
```
6. Run the Streamlit app:
```
streamlit run app.py
```

## ğŸ§ª Tech Stack
- ğŸ§  Ollama (local LLM inference)

- ğŸ” LangChain

- ğŸ—ƒï¸ ChromaDB (in-memory vector store or persistent volume)

- ğŸ Python

- ğŸŒ Streamlit (UI)

- ğŸ“„ PyMuPDF (fitz) for PDF parsing

## ğŸ§± Project Structure
```
â”œâ”€â”€ app.py              # Main Streamlit app
â”œâ”€â”€ rag_utils.py        # PDF parsing & Chroma logic
â”œâ”€â”€ ollama_chat.py      # Querying Ollama with LangChain
â”œâ”€â”€ Dockerfile          # Docker config
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“Œ Future Improvements
- Multi-file support

- Summary generation

- UI themes & dark mode

- Model selection toggle (Mistral, Phi-3, Llama3, etc.)

- Live citation highlighting

- File upload history per session

## ğŸ’¬ Feedback & Contributions
Have ideas or found bugs? Feel free to open an issue or pull request!



